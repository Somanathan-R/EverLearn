{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=28 * 28, hidden_size=256, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, dataloader, importance=1000):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.importance = importance\n",
    "        self.params = {n: p for n, p in model.named_parameters()\n",
    "                       if p.requires_grad}\n",
    "        self.means = {}\n",
    "        self.fisher_matrix = {}\n",
    "\n",
    "        # Compute Fisher Information Matrix\n",
    "        self._compute_fisher_matrix()\n",
    "\n",
    "    def _compute_fisher_matrix(self):\n",
    "        self.model.eval()\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.params.items()}\n",
    "\n",
    "        for inputs, labels in self.dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            self.model.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            for n, p in self.params.items():\n",
    "                fisher[n] += p.grad ** 2 / len(self.dataloader)\n",
    "\n",
    "        self.fisher_matrix = fisher\n",
    "\n",
    "        # Save parameter means\n",
    "        for n, p in self.params.items():\n",
    "            self.means[n] = p.clone()\n",
    "\n",
    "    def penalty(self, model):\n",
    "        penalty = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.fisher_matrix:\n",
    "                fisher = self.fisher_matrix[n]\n",
    "                mean = self.means[n]\n",
    "                penalty += (fisher * (p - mean) ** 2).sum()\n",
    "        return self.importance * penalty\n",
    "\n",
    "# Training function\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, ewc=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if ewc:\n",
    "            loss += ewc.penalty(model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Testing function\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Divide into class incremental tasks\n",
    "tasks = [list(range(i, i + 2))\n",
    "         for i in range(0, 10, 2)]  # [0-1], [2-3], ..., [8-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incremental Learning Loop\n",
    "model = SimpleNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ewc = None\n",
    "\n",
    "for task_idx, task_classes in enumerate(tasks):\n",
    "    print(f\"Task {task_idx + 1}: Classes {task_classes}\")\n",
    "\n",
    "    # Create dataloaders for the current task\n",
    "    task_train_dataset = [d for d in train_dataset if d[1] in task_classes]\n",
    "    task_test_dataset = [d for d in test_dataset if d[1] in task_classes]\n",
    "\n",
    "    task_train_loader = DataLoader(\n",
    "        task_train_dataset, batch_size=64, shuffle=True)\n",
    "    task_test_loader = DataLoader(\n",
    "        task_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train on the current task\n",
    "    for epoch in range(5):  # 5 epochs per task\n",
    "        train_loss = train(model, task_train_loader, optimizer, criterion, ewc)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on all tasks seen so far\n",
    "    for past_task_idx in range(task_idx + 1):\n",
    "        past_task_classes = tasks[past_task_idx]\n",
    "        past_task_test_dataset = [\n",
    "            d for d in test_dataset if d[1] in past_task_classes]\n",
    "        past_task_test_loader = DataLoader(\n",
    "            past_task_test_dataset, batch_size=64, shuffle=False)\n",
    "        accuracy = test(model, past_task_test_loader)\n",
    "        print(\n",
    "            f\"Accuracy on Task {past_task_idx + 1} ({past_task_classes}): {accuracy:.2f}%\")\n",
    "\n",
    "    # Update EWC\n",
    "    ewc = EWC(model, task_train_loader,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incremental Learning Loop\n",
    "model = SimpleNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ewc = None\n",
    "\n",
    "for task_idx, task_classes in enumerate(tasks):\n",
    "    print(f\"Task {task_idx + 1}: Classes {task_classes}\")\n",
    "\n",
    "    # Create dataloaders for the current task\n",
    "    task_train_dataset = [d for d in train_dataset if d[1] in task_classes]\n",
    "    task_test_dataset = [d for d in test_dataset if d[1] in task_classes]\n",
    "\n",
    "    task_train_loader = DataLoader(\n",
    "        task_train_dataset, batch_size=64, shuffle=True)\n",
    "    task_test_loader = DataLoader(\n",
    "        task_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train on the current task\n",
    "    for epoch in range(5):  # 5 epochs per task\n",
    "        train_loss = train(model, task_train_loader, optimizer, criterion, ewc)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on all tasks seen so far\n",
    "    for past_task_idx in range(task_idx + 1):\n",
    "        past_task_classes = tasks[past_task_idx]\n",
    "        past_task_test_dataset = [\n",
    "            d for d in test_dataset if d[1] in past_task_classes]\n",
    "        past_task_test_loader = DataLoader(\n",
    "            past_task_test_dataset, batch_size=64, shuffle=False)\n",
    "        accuracy = test(model, past_task_test_loader)\n",
    "        print(\n",
    "            f\"Accuracy on Task {past_task_idx + 1} ({past_task_classes}): {accuracy:.2f}%\")\n",
    "\n",
    "    # Update EWC\n",
    "    ewc = EWC(model, task_train_loader,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
